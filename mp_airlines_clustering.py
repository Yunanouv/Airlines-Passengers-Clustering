# -*- coding: utf-8 -*-
"""MP-Airlines Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m1SHpdJTVlrQ876XAm59aiTVQh2tGYsY

# Airlines Customer Value Analysis
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import datetime
import plotly.express as px
warnings.filterwarnings('ignore')

# menhhapus limit max column
pd.set_option('display.max_columns', None)

# %matplotlib inline

"""# Load and Describe Data

-------------

## Load Data
"""

dfraw = pd.read_csv('https://drive.google.com/uc?export=download&id=1wQdWcsCZdeStbnx-bx1R2eGMaabjZ9_m')
dfraw.head(3)

# Change the column into lowercase
dfraw.columns = dfraw.columns.str.lower()

"""| Feature Name | Description |
| --- | --- |
| member_no   | Member ID |
| ffp_date    | Frequent Flyer Program Join Date |
| first_flight_date    | First Flight Date |
| gender | Gender |
| ffp_tier | Frequent Flyer Program Tier |
| work_city | Work City |
| work_province | Work Proviince |
| work_country | Work Country |
| age | Customer Age |
| load_time | Date data was taken |
| flight_count | Number of customer flights |
| bp_sum | Total basic integral |
| sum_yr_1 | Fare Revenue|
| sum_yr_2 |  Votes Prices |
| seg_km_sum | Total distance (km) flights that have been done |
| last_flight_date | Last Flight Date |
| last_to_end | last flight time to last flight order interval |
| avg_internal | Average time distance |
| max_interval | Maximum time distance |
| exchange_count | Number of redemptions |
| avg_discount | The average discount that customers get |
| points_sum | The number of points earned by the customer |
| point_notflight | Points not used by members |

## Describe Data
"""

dfraw.info()

"""1. There are 23 features with 62,988 rows of data
2. Features with time information (date) need to be changed from the object to datetime data type
3. The `member_no` feature should be changed from a numeric data type to an object because the number in question is only an ID, not a mathematical operation.
4. The `age` feature should be changed from float to integer data type
5. There are several features that have null values
"""

#Convert int to object
dfraw['member_no'] = dfraw['member_no'].astype(str)

"""Because previously when changing the data type to datetime there was an error as below,  
**ParserError: day is out of range for month: 2014/2/29 0:00:00 present at position 65**

So, we will first check the value in question.
"""

dfraw[dfraw.last_flight_date.str.contains('2014/2/29')]

"""It turns out that there is indeed a different date format written in the `last_flight_date` feature. There are 421 out of 62,988 data or only 0.006%, so we can delete this input error."""

dfraw.drop(dfraw[dfraw.last_flight_date.str.contains('2014/2/29')].index, inplace = True)

# Convert object to datetime
cols = ['ffp_date', 'first_flight_date', 'load_time','last_flight_date']
for col in cols:
    dfraw[col] = pd.to_datetime(dfraw[col])

# Checking null value
dfraw.isnull().sum()

dfraw.dtypes

"""1. There are 6 features; `work_city`, `work_province`, `work_country`, `age`, `sum_yr_1`, and `sum_yr_2` which have null values in them.
2. All features are deemed appropriate according to the data type.
3. For the `age` feature, because there are NA values, the data type will be changed when missing values are handled.
"""

# Checking duplicated rows
dfraw.duplicated().sum()

# Categorize the features
num = ['ffp_tier', 'age', 'flight_count', 'bp_sum', 'sum_yr_1', 'sum_yr_2', 'seg_km_sum', 'last_to_end', 'avg_interval', 'max_interval', 'exchange_count', 'avg_discount', 'points_sum', 'point_notflight']
cat = ['member_no','ffp_date', 'first_flight_date', 'gender', 'work_city', 'work_province', 'work_country', 'load_time', 'last_flight_date']

# Limit number after comma
pd.set_option('display.float_format', lambda x: '%.3f' % x)

dfraw[num].describe()

"""Overall, the dataset does not have a very wide data distribution. Even though there are several features that have big gap of min-max values, this seems reasonable if we compare it with cases in the real world where there are definitely people who fly frequently and on the other hand there are also people who rarely fly.  

Some things that may need attention are:
1. Most numerical features have Mean>Median values and Min-Max values which are quite far apart
2. The `age` feature looks abnormal where the maximum age value is 110 years
3. The `avg_discount` feature also seems to be abnormal, the maximum value is 1.5 or can be interpreted as 150% discount.
4. Fare revenue seen in the `sum_yr_1` and `sum_yr_2` features has a value of 0. This needs to be analyzed further.
"""

dfraw[cat].describe()

"""1. Most of the data is about time (date/time) where things definitely vary which causes the unique value to count a lot. However, there is only 1 unique value in the load_time feature, which is the date the data was taken, that's 3/31/2014.
2. Most passengers come from China and are male.
3. It has been confirmed that there are no duplicate data or that all unique values for `member_no` are correct.
4. The first and last outputs are only visible for features of the datetime type.

--------

# EDA (Exploratory Data Analysis

--------

## Univariate Analysis

### Unique Values
"""

dfraw['ffp_tier'].unique()

"""There are 3 types of tiers; 4, 5, and 6."""

dfraw['work_city'].unique()

dfraw['work_province'].unique()

"""There are categorical features that require data cleaning in the form of generalizing string writing.

### Data Distribution
"""

# Graph of Data Distribution

plt.figure(figsize=(20, 16))
for i in range(0, len(num)):
    plt.subplot(4, 4, i+1)
    sns.distplot(dfraw[num[i]], color='blue')
    plt.tight_layout()

"""### Outliers"""

# Checking Outliers
plt.figure(figsize=(12, 6))
for i in range(0, len(num)):
    plt.subplot(2, 7, i+1)
    sns.boxplot(dfraw[num[i]], color='red', orient='v')
    plt.title(num[i])
    plt.tight_layout()

"""1. The `ffp_tier` feature basically has no outliers because it only has 3 values, namely 4, 5, and 6.
2. Other features have right-skewed distributions and outliers.
3. Almost all features have outliers. This may be handled by removing extreme values and also proper feature selection.

### Abnormal Value

**1. Avg Discount**
"""

# Creating a Discount Avg Plot that has odd values
plt.figure(figsize=(6, 4))
sns.distplot(dfraw.avg_discount)
plt.title('Avg Discount')

dfraw[dfraw['avg_discount'] > 1.0]

dfraw[dfraw['avg_discount'] == 0.0]

"""Even getting a 100% discount may be rare, especially if you get a discount of more than 100% (read: > 1.0). There are 3684 customers who get a discount >=1. If we assume that the discount was obtained because of the flights they took, then that cannot be said to be true because some of these customers also have low total points and a long distance from the last flight. Therefore, because there are only (2927+8) of the 62k++ data (0.06%) this data will be considered an input error and will be deleted.

The same thing also seems unnatural with an avg_discount of 0, where the total flight (`seg_km_sum`) is not 0 and some have points (`points_sum`). So, this data will be deleted because it is considered unnatural and an input error.

**2. Age**
"""

plt.figure(figsize=(30,6))
sns.set(style='whitegrid')
ax = sns.countplot(x='age', width = 0.5, data=dfraw, palette='rocket')
ax.set_title('Age Distribution', fontsize=8, fontweight='bold')
ax.set_xlabel('Age', fontsize=7, fontweight='medium')

dfraw[dfraw.age > 100]

"""Customers aged between 28-56 years tend to frequently travel.
However, it is also abnormal if there are customers who are more than 100 years old, so we assume that this is an input error that falls into the outliers category.

**3. Fare Revenue**
"""

dfraw[(dfraw.sum_yr_1 == 0) & (dfraw.sum_yr_2 == 0) & (dfraw.seg_km_sum > 0)]

"""We found another indication of an input error where fare revenue `sum_yr_1`, `sum_yr_2` have a value of 0 even though the passenger traveled `seg_km_sum`. This is certainly abnormal if there is travel but no income. Because there are only 245 of 52931 data (0.004%) data, we will delete them.

## Multivariate Analysis

### Correlation Heatmap
"""

# Correlation Heatmap
corr_matrix = dfraw.corr()
plt.figure(figsize=(8, 8))
sns.heatmap(corr_matrix, annot=True, annot_kws={'size': 10}, fmt='.2f', cmap='coolwarm', center=0)
plt.title('Correlation Heatmap')
plt.show()

"""1. Features `age`, `last_to_end`, `avg_interval`, `max_interval`, `avg_discount`, `point_notflight` have a low correlation with other features (<0.5)
2. Features that have a high correlation are `flight_count`, `bp_sum`, `sum_yr_1`, `sum_yr_2`, `seg_km_sum`, `points_sum`.
3. Features `bp_sum`and `points_sum` are more or less the same, we can say the correlation with `seg_km_sum` implies that customers can gain more points by having larger total mileage.

### Fare Revenue from Flight Count Based on Age
"""

# Visualization of Fare Revenue from Flight Count Based on Age

plt.figure(figsize=(12, 6))
scatter = sns.scatterplot(data=dfraw, x='sum_yr_1', y='flight_count', hue='age', size='age', palette='coolwarm')
plt.title('Fare Revenue')
plt.show()

"""1. The same as the previous analysis where the age of customers who actively travel is in their 20s and up to 60 years.
2. The amount of fare revenue obtained is also directly proportional to the number of flights carried out.
3. The most common fare revenue is below 50,000.

----------

# Data Preparation

----------

## Data Cleansing

### Dropping

**Duplicated Rows**

In the Data Exploration, it was seen that there were no duplicate rows.

### Rows with Abnormal Value (Outliers)

In the previous stage, we marked features that have abnormal values, so they need to be checked and handled further because they could be indicated as outliers. These values are:
1. `avg_discount` which has a value of 0 and more than 1.
2. `age` which is 110 or more than 100 years old.
3. `sum_yr_1` and `sum_yr_2` which have fare revenue 0.
"""

df_clean = dfraw.copy()
df_clean.head(3)

# Drop 0 avg_discount
nol_disc = df_clean[((df_clean.avg_discount == 0.0))].index
df_clean = df_clean.drop(nol_disc)

# Drop >1 avg_discount
satu_disc = df_clean[((df_clean.avg_discount > 1.0))].index
df_clean = df_clean.drop(satu_disc)

# Drop >100 age
abn_age = df_clean[((df_clean.age > 100))].index
df_clean = df_clean.drop(abn_age)

# Drop rows
abn_fare = df_clean[(df_clean.sum_yr_1 == 0) & (df_clean.sum_yr_2 == 0) & (df_clean.seg_km_sum > 0)].index
df_clean = df_clean.drop(abn_fare)

df_clean.shape

"""### Handle Missing Value"""

# Re-checking null value
df_clean.isnull().sum()

"""There are 7 features that have Null values, they are `gender`, `work_city`, `work_province`, `work_country`, `age`, `sum_yr_1`, `sum_yr_2`.

1. The `gender` feature which only has 1 null value will be removed.
2. The `work_city`, `work_province`, `work_country` features will be filled with mode values.
2. `age` which has a fairly normal data distribution, will be filled with the mean value.
3. `sum_yr_1` and `sum_yr_2` will be filled with median values.
"""

mode_value = df_clean.filter(['work_city', 'work_province', 'work_country']).mode()
cols = ['work_city', 'work_province', 'work_country']

df_clean[cols] = df_clean[cols].fillna(df_clean.mode().iloc[0])

df_clean['age'] = df_clean['age'].fillna(df_clean['age'].mean())
df_clean['sum_yr_1'] = df_clean['sum_yr_1'].fillna(df_clean['sum_yr_1'].median())
df_clean['sum_yr_2'] = df_clean['sum_yr_2'].fillna(df_clean['sum_yr_2'].median())

df_clean.dropna(axis=0, inplace=True)
df_clean.isnull().sum()

df_clean['age'] = df_clean['age'].astype(int)

"""## Feature Engineering

#### Duration of Being a Member

Extracted to find out how long each customer has joined the FFP (Frequent Flyer Program) program by calculating the distance from `load_time` (data taken) and `ffp_date` (when joined).
"""

df_clean['day_as_member'] = (df_clean['load_time'] - df_clean['ffp_date']).dt.days
df_clean.head(2)

df_clean.shape

"""## LRFM ANALYSIS

The Length, Recency, Frequency, and Monetary model, also known as the LRFM model, was introduced as an improved version of the RFM model to identify more relevant and exact consumer groups for profit maximization.

* **Length** it shows how long the customer has been a member <br>
  `day_as_member` : Selected because it tells how many days as member
* **Recency** refers to the last time a user made a transaction <br>
`last_to_end`: Selected because it contains information about the difference in days between the data collection date and the last flight date <br>
* **Frequency** refers to how often customers make transactions <br>
`flight_count` : Selected because it contains data on the number of flights the customer has taken <br>
* **Monetary** value refers to how much each user spends on the entire transaction <br>
`seg_km_sum` : Selected because it contains data on the total flight distance that each customer has traveled which can describe the number of transactions and expenses that have been incurred.  
 `points_sum`: Selected because it contains the number of points owned by each customers which are generally obtained every time they make a transaction

#### Handle Outliers
"""

def outlier_del(df, column, mode):
    q1 = df.iloc[:,column].quantile(0.25)
    q3 = df.iloc[:,column].quantile(0.75)
    iqr = q3-q1
    lower_tail = q1 - (1.5 * iqr)
    upper_tail = q3 + (1.5 * iqr)
    column_name = df.columns[column]
    total_outliers = df[(df.iloc[:,column] <= lower_tail)|(df.iloc[:,column] >= upper_tail)].iloc[:,column].count()
    total_row = df.iloc[:,column].count()
    percent_outliers = round(((total_outliers/total_row)*100),2)
    if mode == 'summary':
        return print('Total Outliers in ', column_name, ' :', total_outliers, ' and outliers percentage:', percent_outliers, '%')
    elif mode == 'df':
        return df[(df.iloc[:,column] >= lower_tail)&(df.iloc[:,column] <= upper_tail)]
    else :
        return print('Check the input')

# Checking outliers percentage
column = [10, 14, 16, 21, 23]

for i in range(0, len(column)):
    outlier_del(df_clean, column[i], 'summary')

# Delete Outlier
df_clean = df_clean[df_clean.index.isin(outlier_del(df_clean, 10, 'df').reset_index()['index'])]
df_clean = df_clean[df_clean.index.isin(outlier_del(df_clean, 14, 'df').reset_index()['index'])]
df_clean = df_clean[df_clean.index.isin(outlier_del(df_clean, 16, 'df').reset_index()['index'])]
df_clean = df_clean[df_clean.index.isin(outlier_del(df_clean, 21, 'df').reset_index()['index'])]

print(f'Total rows after removing outliers: {len(df_clean)}')

"""### Feature Selection"""

df_select = df_clean.copy()

df_select = df_select[['flight_count', 'seg_km_sum', 'last_to_end', 'points_sum', 'day_as_member']]
df_select.head(2)

"""## Data Scaling"""

from sklearn.preprocessing import StandardScaler

# Standardize the data
scaler = StandardScaler()
feat_std = scaler.fit_transform(df_select)

# Inserting normalization results into DataFrame df std
for i, col in enumerate(df_select):
    df_select['std_' + col] = feat_std[:, i]

df_select.describe()

df_std = df_select.copy()
df_std.drop(columns= ['last_to_end', 'flight_count', 'seg_km_sum', 'day_as_member', 'points_sum'], inplace= True)
df_std.head(3)

"""------------

# Clustering

------------

## Inertia Check
"""

from sklearn.cluster import KMeans

# Calculates inertia values for 2 to 10 clusters
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=0)
    kmeans.fit(df_std)
    inertia.append(kmeans.inertia_)


# Visualize inertia
sns.set_style('white')
plt.figure(figsize= (10, 5))
sns.lineplot(x= range(1, 11), y= inertia, marker='o', color = '#000087', linewidth = 3)
sns.scatterplot(x=range(1, 11), y=inertia, s=300, color='#800000',  linestyle='--')
plt.title('Visualization of Inertia')

# See the difference in inertia percentage for each additional cluster

((pd.Series(inertia) - pd.Series(inertia).shift(-1)) / pd.Series(inertia) * 100).dropna()

"""## Silhouette Score"""

from sklearn.metrics import silhouette_score

# Silhouette score for 2 to 10 clusters
range_n_clusters = list(range(2,11))
arr_silhouette_score_euclidean = []
for i in range_n_clusters:
    kmeans = KMeans(n_clusters=i).fit(df_std)
    cluster_labels = kmeans.predict(df_std)

    score_euclidean = silhouette_score(df_std, cluster_labels, metric='euclidean')
    arr_silhouette_score_euclidean.append(score_euclidean)

# Plot Silhouette Score
sns.set_style('white')
plt.figure(figsize=(8, 6))
sns.lineplot(x=range(2,11), y=arr_silhouette_score_euclidean,  marker='o', color='#000087', linewidth = 4)
sns.scatterplot(x=range(2,11), y=arr_silhouette_score_euclidean, s=300, color='#800000',  linestyle='--')
plt.xlabel('Jumlah Cluster')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score')
plt.show()

"""From the Silhouette score results above, it can be seen that the optimal number of clusters is 4 clusters.

## Cluster Evaluation
"""

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, random_state=0).fit(df_std)

clusters = kmeans.labels_

# PCA and Visualization of Clusters

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca.fit(df_std)
pcs = pca.transform(df_std)

df_pca = pd.DataFrame(data = pcs, columns = ['PC 1', 'PC 2'])
df_pca.head()

# Visualize the Clusters Using Silhouette Visualizer
from yellowbrick.cluster import SilhouetteVisualizer

model = KMeans(4)
visualizer = SilhouetteVisualizer(model)

visualizer.fit(df_pca)
visualizer.show()

"""From the picture above, it can be seen that all clusters have good coefficient values. This means that the model created is very ideal."""

df_pca['clusters'] = clusters
df_pca.sample(5)

# Visualization of cluster distribution

fig, ax = plt.subplots(figsize=(10,6))

sns.scatterplot(
    x='PC 1', y='PC 2',
    hue='clusters',
    linestyle='--',
    data=df_pca,
    marker = '+',
    palette=['blue','red','green','yellow'],
    s=50,
    ax=ax
)

"""**Assign CLuster Label to the Original Dataset**"""

# Assign cluster to dataset
df_clean.loc[:,'clusters'] = kmeans.labels_
df_clean.sample(5)

df_clean['clusters'].value_counts().to_frame().reset_index().rename(columns={"index": "clusters", "clusters": "total_members"})

# Show the statistics of each cluster
df_select['clusters'] = clusters
display(df_select.groupby('clusters').agg(['min', 'max','mean','median']))

"""## About the Clusters

The explanation of each cluster is as follows:

#### Cluster 0 :
Customers in this cluster are more likely to take a stable number of flights with an average of 5–6 flights over relatively short distances and an average of 122 points. This cluster has a membership period that is not too long or can be said to have just joined among other cluster customers.

#### Cluster 1:
Customers in this cluster have characteristics like cluster 0, but the average points obtained are higher than cluster 0, that is 146 points. Customers who are members of this cluster are also not old members or can be said to have just joined. Maybe because there is only a small amount of data recorded (flight data since the program was input) which is why clusters 0 and 1 still don't have a high number of flights.
These two clusters have the potential to be upgraded more because the number of flights they have is quite good even though they have just joined.

#### Cluster 2:
Customers belonging to cluster 2 are those who make fewer travels over relatively short distances, but have high points. In addition, they have joined  the program longer than clusters 0 and 1. This means they have good loyalty because they continue to join the membership but rarely travel.

#### Cluster 3:
The best customers are customers in cluster 3. This cluster has a higher number of flights than other clusters with an average of 15-16 flights over long distances. It is likely that this cluster's customers often travel internationally. Apart from that, the number of points they get is also high because they have been members for a long time.

## Business Recommendation

### Cluster 0 dan 1 (Potential Customers)  
Because clusters 0 and 1 are potential customers where they often travel even though they have just joined as members, the focus of increasing revenue is to retain them so they continue to be members and provide attractive rewards so they continue to travel frequently. So the business recommendations are:  
**1. Birthday Coupon**  
Providing discounts on flights in customers' birthday months of up to 10% according to the number of points they get. Another advantage is that if they travel on their birthday, they will get double points.  
**2. Points Redemption**  
Every customer who has reached a certain number of points has the opportunity to redeem their points for shopping coupons or lodging coupons at company partners.  
**3. Tour Package Affiliates**  
Offers an affiliate program where customers who succeed in bringing at least 4 people to become members will get a discount on the price of the tour package with the people they invite.   

### Cluster 2 (Loyal Customers)  
Because cluster 2 is a cluster where customers have been members for a long time but rarely travel, our focus is on how to get them to travel frequently. Some business recommendations are:  
**1. Inactive Treatment**  
Customers who have not traveled for up to 6 months will be given a special offer if they travel between the specified time limits, then they will get a 10% discount and double points. This aims to minimize inactive customers.  
**2. Couple Package**  
Create a special travel package for 2 people with your partner, friends or others. Each of them will get a 5% discount and double points. This aims to ensure that many passengers travel every day.  
**3. Tour Package**  
Special offer for those who travel and want to have recreation. They will get special discounts and good service from company partners.  

### Cluster 3 (Exclusive Customers)  
Of course, customers in cluster 3 are exclusive customers who must be looked after well. They spend money on the company so the focus of increasing business is to keep them using our flights. Some business recommendations are:  
**1. First Priority**  
Make customers of this cluster a top priority, such as priority seats, free food and drinks, and bigger discounts than other clusters.  
**2. Luxury Service**  
Luxury international travel promotions. Offer luxury travel packages with premium amenities such as five-star accommodations, first-class flights and personalized personal service.  
**3. Half Price Refund**  
The special advantage that this cluster has apart from priority and luxury service is the opportunity to get a 100% refund on the same day with a certain minimum number of points.

### For all clusters  
**1. Developing mobile applications**   
Developing mobile applications to make it easier for customers to access flight information, points and exclusive offers that can only be optimally used through the application.   
**2. Gamification**  
Give class upgrades or special facilities to customers who have reached certain points to be able to enjoy promos or small discounts.  
**3. Seasons Promotion**  
Plan seasonal promotions that align with holidays and special events, such as religious holiday discounts, special New Year's offers, or other holidays.
"""